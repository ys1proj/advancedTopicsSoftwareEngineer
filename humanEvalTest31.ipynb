{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 6\n",
      "Model Output: False\n",
      "Expected: False\n",
      "Pass: True\n",
      "\n",
      "Input: 101\n",
      "Model Output: False\n",
      "Expected: True\n",
      "Pass: False\n",
      "\n",
      "Input: 11\n",
      "Model Output: False\n",
      "Expected: True\n",
      "Pass: False\n",
      "\n",
      "Input: 13441\n",
      "Model Output: False\n",
      "Expected: True\n",
      "Pass: False\n",
      "\n",
      "Input: 61\n",
      "Model Output: False\n",
      "Expected: True\n",
      "Pass: False\n",
      "\n",
      "Input: 4\n",
      "Model Output: False\n",
      "Expected: False\n",
      "Pass: True\n",
      "\n",
      "Input: 1\n",
      "Model Output: False\n",
      "Expected: False\n",
      "Pass: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_name = \"finegptproject/humaneval_SFTTrainer_model\"\n",
    "\n",
    "### HUMANEVAL TEST 31 ####\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def generate_response(prompt: str) -> str:\n",
    "    \"\"\"Generate a response from the model given a prompt.\"\"\"\n",
    "    inputs_encoded = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate the output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs_encoded, max_length=100)\n",
    "    \n",
    "    output_str = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return output_str\n",
    "\n",
    "def run_test_cases(test_cases):\n",
    "    for inputs, expected in test_cases:\n",
    "        # Convert inputs to string representation for the model\n",
    "        input_str = f\"Is the number {inputs} prime? Answer with True or False.\"\n",
    "        \n",
    "        # Generate the output\n",
    "        output_str = generate_response(input_str)\n",
    "        \n",
    "        # Convert output to boolean and check if it matches expected result\n",
    "        try:\n",
    "            result = output_str.lower() in [\"true\", \"true.\"]\n",
    "            pass_status = result == expected\n",
    "        except ValueError:\n",
    "            result = None\n",
    "            pass_status = False\n",
    "\n",
    "        print(f\"Input: {inputs}\")\n",
    "        print(f\"Model Output: {result}\")\n",
    "        print(f\"Expected: {expected}\")\n",
    "        print(f\"Pass: {pass_status}\\n\")\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (6, False),        # 6 is not a prime number\n",
    "    (101, True),       # 101 is a prime number\n",
    "    (11, True),        # 11 is a prime number\n",
    "    (13441, True),     # 13441 is a prime number\n",
    "    (61, True),        # 61 is a prime number\n",
    "    (4, False),        # 4 is not a prime number\n",
    "    (1, False)         # 1 is not a prime number\n",
    "]\n",
    "\n",
    "# Run the test cases\n",
    "run_test_cases(test_cases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ysch2\\Documents\\GitHub\\advancedTopicsSoftwareEngineer\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 6\n",
      "Model Output: False\n",
      "Expected: False\n",
      "Pass: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 101\n",
      "Model Output: False\n",
      "Expected: True\n",
      "Pass: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 11\n",
      "Model Output: False\n",
      "Expected: True\n",
      "Pass: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 13441\n",
      "Model Output: False\n",
      "Expected: True\n",
      "Pass: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 61\n",
      "Model Output: False\n",
      "Expected: True\n",
      "Pass: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 4\n",
      "Model Output: False\n",
      "Expected: False\n",
      "Pass: True\n",
      "\n",
      "Input: 1\n",
      "Model Output: False\n",
      "Expected: False\n",
      "Pass: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"  # This time check with GPT2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "def generate_response(prompt: str) -> str:\n",
    "    \"\"\"Generate a response from the model given a prompt.\"\"\"\n",
    "    inputs_encoded = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate the output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs_encoded, max_length=100)\n",
    "    \n",
    "    output_str = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return output_str\n",
    "\n",
    "def run_test_cases(test_cases):\n",
    "    for inputs, expected in test_cases:\n",
    "        # Convert inputs to string representation for the model\n",
    "        input_str = f\"Is the number {inputs} prime? Answer with True or False.\"\n",
    "        \n",
    "        # Generate the output\n",
    "        output_str = generate_response(input_str)\n",
    "        \n",
    "        # Convert output to boolean and check if it matches expected result\n",
    "        try:\n",
    "            result = output_str.lower() in [\"true\", \"true.\"]\n",
    "            pass_status = result == expected\n",
    "        except ValueError:\n",
    "            result = None\n",
    "            pass_status = False\n",
    "\n",
    "        print(f\"Input: {inputs}\")\n",
    "        print(f\"Model Output: {result}\")\n",
    "        print(f\"Expected: {expected}\")\n",
    "        print(f\"Pass: {pass_status}\\n\")\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (6, False),        # 6 is not a prime number\n",
    "    (101, True),       # 101 is a prime number\n",
    "    (11, True),        # 11 is a prime number\n",
    "    (13441, True),     # 13441 is a prime number\n",
    "    (61, True),        # 61 is a prime number\n",
    "    (4, False),        # 4 is not a prime number\n",
    "    (1, False)         # 1 is not a prime number\n",
    "]\n",
    "\n",
    "# Run the test cases\n",
    "run_test_cases(test_cases)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
